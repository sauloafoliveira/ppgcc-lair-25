## Tema da Pesquisa

**Compressão de Modelos Transformers para NLP**

Este projeto tem como objetivo avaliar e comparar técnicas de compressão aplicadas a modelos Transformers em tarefas de Processamento de Linguagem Natural (NLP) em inglês. O foco está na análise do desempenho desses modelos comprimidos em ambientes com recursos computacionais limitados, como dispositivos embarcados, CPUs sem GPU dedicada ou servidores de baixo custo. A pesquisa contribui para o avanço do uso eficiente de modelos de linguagem em contextos com restrições de hardware, promovendo acessibilidade e sustentabilidade no uso da IA.


## Estrutura do Repositório

O repositório está organizado conforme os principais marcos do percurso de pesquisa no mestrado: **Qualificação** e **Defesa**. Além disso, são destacados os principais produtos esperados: **Artigo de Revisão Sistemática**, **Artigo para a Dissertação** e o **Documento da Dissertação**.

## Referências Principais

- SANH, Victor et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv, 2019.
- TURC, Alex et al. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models. arXiv, 2019.
- MICHEL, Paul et al. Are Sixteen Heads Really Better than One? NeurIPS, 2019.
- ROGERS, Anna et al. A Primer in BERTology: What We Know About How BERT Works. TACL, 2020.
- GOODFELLOW, Ian; BENGIO, Yoshua; COURVILLE, Aaron. Deep Learning. MIT Press, 2016.

## Como Citar

Se utilizar materiais, scripts ou resultados deste repositório, por favor, cite conforme o seguinte modelo:

> "Este material foi produzido no contexto do Mestrado em Ciência da Computação, sob orientação do Prof. Dr. Saulo Anderson Freitas de Oliveira, ccom foco na compressão de modelos de linguagem para uso eficiente em ambientes com baixo poder computacional."
